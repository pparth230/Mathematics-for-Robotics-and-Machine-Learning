# Mathematics-for-Robotics-and-Machine-Learning

## ğŸ¯ Foundation: Algebra & Pre-Calculus

### Linear Equations
- [ ] **Equation**: `ax + b = 0`
- [ ] **Application**: Solving for joint positions

### Quadratic Formula
- [ ] **Equation**: `x = (-b Â± âˆš(bÂ²-4ac))/2a`
- [ ] **Application**: Trajectory parabolas

### Exponentials
- [ ] **Equation**: `e^x`, `a^x`
- [ ] **Application**: Discount factors `Î³^t`

### Logarithms
- [ ] **Equation**: `log(xy) = log(x) + log(y)`
- [ ] **Application**: Log-likelihood optimization

### Trigonometry
- [ ] **Equation**: `sinÂ²Î¸ + cosÂ²Î¸ = 1`
- [ ] **Application**: Rotation matrices

---

## ğŸ“ Linear Algebra (Level 1)

### Vector Operations
- [ ] **Vector addition**: `v + w = [vâ‚+wâ‚, vâ‚‚+wâ‚‚, ...]`
- [ ] **Application**: State combinations

### Scalar Multiplication
- [ ] **Equation**: `cv = [cvâ‚, cvâ‚‚, ...]`
- [ ] **Application**: Scaling actions

### Dot Product
- [ ] **Equation**: `vÂ·w = Î£váµ¢wáµ¢`
- [ ] **Application**: Neuron activation

### Matrix Multiplication
- [ ] **Equation**: `(AB)áµ¢â±¼ = Î£â‚– Aáµ¢â‚–Bâ‚–â±¼`
- [ ] **Application**: Layer transformations

### Matrix Transpose
- [ ] **Equation**: `(Aáµ€)áµ¢â±¼ = Aâ±¼áµ¢`
- [ ] **Application**: Backpropagation gradients

### Identity Matrix
- [ ] **Equation**: `Iv = v`
- [ ] **Application**: No transformation

### Matrix Inverse
- [ ] **Equation**: `AAâ»Â¹ = I`
- [ ] **Application**: Inverse kinematics

---

## ğŸ“ˆ Calculus (Single Variable)

### Limits
- [ ] **Equation**: `lim(xâ†’a) f(x)`
- [ ] **Application**: Convergence checks

### Derivatives
- [ ] **Equation**: `f'(x) = lim(hâ†’0) [f(x+h)-f(x)]/h`
- [ ] **Application**: Gradient computation

### Power Rule
- [ ] **Equation**: `d/dx(xâ¿) = nxâ¿â»Â¹`
- [ ] **Application**: Polynomial derivatives

### Chain Rule
- [ ] **Equation**: `d/dx[f(g(x))] = f'(g(x))Â·g'(x)`
- [ ] **Application**: Backpropagation

### Product Rule
- [ ] **Equation**: `d/dx[f(x)g(x)] = f'(x)g(x) + f(x)g'(x)`
- [ ] **Application**: Complex derivations

### Integration
- [ ] **Equation**: `âˆ«f(x)dx`
- [ ] **Application**: Cumulative reward

### Fundamental Theorem of Calculus
- [ ] **Equation**: `âˆ«â‚áµ‡ f'(x)dx = f(b) - f(a)`
- [ ] **Application**: Total change calculation

---

## ğŸ“ Linear Algebra (Level 2)

### Determinant
- [ ] **Equation**: `det(A) = ad - bc` (2Ã—2 case)
- [ ] **Application**: Area scaling, invertibility

### Eigenvalues and Eigenvectors
- [ ] **Equation**: `Av = Î»v`
- [ ] **Application**: Stability analysis

### Characteristic Equation
- [ ] **Equation**: `det(A - Î»I) = 0`
- [ ] **Application**: Finding eigenvalues

### Singular Value Decomposition (SVD)
- [ ] **Equation**: `A = UÎ£Váµ€`
- [ ] **Application**: Dimensionality reduction

### Frobenius Norm
- [ ] **Equation**: `||A||_F = âˆš(Î£áµ¢â±¼ aáµ¢â±¼Â²)`
- [ ] **Application**: Matrix distance

### Vector Norm
- [ ] **Equation**: `||v|| = âˆš(Î£váµ¢Â²)`
- [ ] **Application**: Euclidean distance

---

## ğŸ² Multivariable Calculus

### Partial Derivatives
- [ ] **Equation**: `âˆ‚f/âˆ‚xáµ¢`
- [ ] **Application**: Gradient components

### Gradient Vector
- [ ] **Equation**: `âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]`
- [ ] **Application**: Steepest ascent direction

### Chain Rule (Multivariate)
- [ ] **Equation**: `âˆ‚z/âˆ‚x = (âˆ‚z/âˆ‚y)(âˆ‚y/âˆ‚x)`
- [ ] **Application**: Neural network gradients

### Jacobian Matrix
- [ ] **Equation**: `Jáµ¢â±¼ = âˆ‚fáµ¢/âˆ‚xâ±¼`
- [ ] **Application**: Robot velocity relationships

### Hessian Matrix
- [ ] **Equation**: `Háµ¢â±¼ = âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼`
- [ ] **Application**: Curvature information

### Directional Derivative
- [ ] **Equation**: `D_v f = âˆ‡fÂ·v`
- [ ] **Application**: Gradient in direction

### Double Integrals
- [ ] **Equation**: `âˆ«âˆ«f(x,y)dxdy`
- [ ] **Application**: 2D probability mass

---

## ğŸ”„ Differential Equations

### First-Order ODEs
- [ ] **Equation**: `dy/dt = f(t,y)`
- [ ] **Application**: Velocity from acceleration

### Second-Order ODEs
- [ ] **Equation**: `dÂ²y/dtÂ² = f(t,y,dy/dt)`
- [ ] **Application**: Newton's second law

### Linear ODEs
- [ ] **Equation**: `dy/dt + p(t)y = g(t)`
- [ ] **Application**: Damped systems

### Exponential Solutions
- [ ] **Equation**: `y(t) = yâ‚€e^(kt)`
- [ ] **Application**: Growth/decay models

### Euler Method
- [ ] **Equation**: `yâ‚™â‚Šâ‚ = yâ‚™ + hÂ·f(tâ‚™,yâ‚™)`
- [ ] **Application**: Basic simulation step

### Runge-Kutta 4th Order (RK4)
- [ ] **Equation**: Complex 4-stage formula
- [ ] **Application**: Accurate physics simulation

### Stability Condition
- [ ] **Equation**: `Re(Î») < 0`
- [ ] **Application**: System convergence

---

## ğŸ² Probability (Level 1)

### Probability Axioms
- [ ] **Equation**: `0 â‰¤ P(A) â‰¤ 1`, `P(Î©) = 1`
- [ ] **Application**: Valid probability measures

### Addition Rule
- [ ] **Equation**: `P(AâˆªB) = P(A) + P(B) - P(Aâˆ©B)`
- [ ] **Application**: Union probability

### Conditional Probability
- [ ] **Equation**: `P(A|B) = P(Aâˆ©B)/P(B)`
- [ ] **Application**: Bayesian updates

### Independence
- [ ] **Equation**: `P(Aâˆ©B) = P(A)P(B)`
- [ ] **Application**: Feature independence assumption

### Bayes' Theorem
- [ ] **Equation**: `P(A|B) = P(B|A)P(A)/P(B)`
- [ ] **Application**: Posterior estimation

### Law of Total Probability
- [ ] **Equation**: `P(A) = Î£P(A|Báµ¢)P(Báµ¢)`
- [ ] **Application**: Marginalization

---

## ğŸ“Š Statistics (Level 1)

### Expectation
- [ ] **Equation**: `E[X] = Î£xÂ·P(X=x)` or `âˆ«xÂ·f(x)dx`
- [ ] **Application**: Mean reward

### Variance
- [ ] **Equation**: `Var(X) = E[(X-Î¼)Â²] = E[XÂ²] - E[X]Â²`
- [ ] **Application**: Uncertainty measure

### Standard Deviation
- [ ] **Equation**: `Ïƒ = âˆšVar(X)`
- [ ] **Application**: Spread measure

### Gaussian (Normal) Distribution
- [ ] **Equation**: `f(x) = (1/âˆš(2Ï€ÏƒÂ²))e^(-(x-Î¼)Â²/2ÏƒÂ²)`
- [ ] **Application**: Noise modeling

### Uniform Distribution
- [ ] **Equation**: `f(x) = 1/(b-a)` for `xâˆˆ[a,b]`
- [ ] **Application**: Random exploration

### Law of Large Numbers
- [ ] **Equation**: `XÌ„â‚™ â†’ Î¼` as `nâ†’âˆ`
- [ ] **Application**: Sample average converges

### Central Limit Theorem
- [ ] **Equation**: `âˆšn(XÌ„â‚™-Î¼)/Ïƒ â†’ N(0,1)`
- [ ] **Application**: Sampling distributions

---

## ğŸ² Probability (Level 2)

### Joint Probability Density
- [ ] **Equation**: `f(x,y)`
- [ ] **Application**: Multi-sensor measurements

### Marginal Distribution
- [ ] **Equation**: `f_X(x) = âˆ«f(x,y)dy`
- [ ] **Application**: Integrating out variables

### Covariance
- [ ] **Equation**: `Cov(X,Y) = E[(X-Î¼â‚“)(Y-Î¼áµ§)]`
- [ ] **Application**: Correlation measure

### Correlation Coefficient
- [ ] **Equation**: `Ï = Cov(X,Y)/(Ïƒâ‚“Ïƒáµ§)`
- [ ] **Application**: Normalized correlation

### Multivariate Gaussian
- [ ] **Equation**: `f(x) = (2Ï€)^(-n/2)|Î£|^(-1/2)e^(-Â½(x-Î¼)áµ€Î£â»Â¹(x-Î¼))`
- [ ] **Application**: State uncertainty representation

### Conditional Independence
- [ ] **Equation**: `P(A|B,C) = P(A|C)`
- [ ] **Application**: Graphical model simplification

---

## ğŸ“Š Statistics (Level 2)

### Likelihood Function
- [ ] **Equation**: `L(Î¸|x) = P(x|Î¸)`
- [ ] **Application**: Data given parameters

### Log-Likelihood
- [ ] **Equation**: `â„“(Î¸) = log L(Î¸) = Î£log P(xáµ¢|Î¸)`
- [ ] **Application**: Easier optimization

### Maximum Likelihood Estimation (MLE)
- [ ] **Equation**: `Î¸Ì‚ = argmax L(Î¸|x)`
- [ ] **Application**: Parameter estimation

### Maximum A Posteriori (MAP)
- [ ] **Equation**: `Î¸Ì‚ = argmax P(Î¸|x) = argmax P(x|Î¸)P(Î¸)`
- [ ] **Application**: Bayesian parameter estimation

### Posterior Distribution
- [ ] **Equation**: `P(Î¸|x) âˆ P(x|Î¸)P(Î¸)`
- [ ] **Application**: Updated beliefs

### Confidence Intervals
- [ ] **Equation**: `[Î¸Ì‚ - z*SE, Î¸Ì‚ + z*SE]`
- [ ] **Application**: Uncertainty quantification

---

## ğŸ¯ Optimization (Level 1)

### Gradient Descent
- [ ] **Equation**: `Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±âˆ‡L(Î¸â‚œ)`
- [ ] **Application**: Parameter updates

### Learning Rate Schedule
- [ ] **Equation**: `Î±â‚œ = Î±â‚€/(1+decayÂ·t)`
- [ ] **Application**: Adaptive step size

### Momentum
- [ ] **Equation**: `vâ‚œ = Î²vâ‚œâ‚‹â‚ + âˆ‡L(Î¸â‚œ)`, `Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±vâ‚œ`
- [ ] **Application**: Accelerated gradient descent

### Adam Optimizer
- [ ] **Equation**: `mâ‚œ = Î²â‚mâ‚œâ‚‹â‚ + (1-Î²â‚)âˆ‡L`, `vâ‚œ = Î²â‚‚vâ‚œâ‚‹â‚ + (1-Î²â‚‚)âˆ‡LÂ²`
- [ ] **Application**: Adaptive learning rates

### Line Search
- [ ] **Equation**: `minimize f(x + Î±d)` over `Î±`
- [ ] **Application**: Optimal step size

### Convergence Criterion
- [ ] **Equation**: `||âˆ‡L(Î¸â‚œ)|| < Îµ`
- [ ] **Application**: Training termination

---

## ğŸ¯ Optimization (Level 2)

### Lagrangian
- [ ] **Equation**: `â„’(x,Î») = f(x) + Î£Î»áµ¢gáµ¢(x)`
- [ ] **Application**: Constrained optimization

### KKT Conditions
- [ ] **Equation**: `âˆ‡f + Î£Î»áµ¢âˆ‡gáµ¢ = 0`, `Î»áµ¢gáµ¢ = 0`
- [ ] **Application**: Optimal constrained solution

### Quadratic Programming
- [ ] **Equation**: `min Â½xáµ€Qx + cáµ€x` subject to `Axâ‰¤b`
- [ ] **Application**: Trajectory optimization

### Newton's Method
- [ ] **Equation**: `Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Hâ»Â¹âˆ‡L`
- [ ] **Application**: Second-order optimization

### Convexity Condition
- [ ] **Equation**: `âˆ‡Â²f â‰½ 0` (positive semi-definite Hessian)
- [ ] **Application**: Guaranteed global minimum

---

## ğŸ“ Linear Algebra (Advanced)

### Inner Product
- [ ] **Equation**: `âŸ¨u,vâŸ© = uáµ€v`
- [ ] **Application**: Generalized dot product

### Orthogonality
- [ ] **Equation**: `âŸ¨u,vâŸ© = 0`
- [ ] **Application**: Perpendicular vectors

### Vector Projection
- [ ] **Equation**: `proj_v(u) = (âŸ¨u,vâŸ©/||v||Â²)v`
- [ ] **Application**: Component extraction

### Gram-Schmidt Process
- [ ] **Equation**: `vâ‚– = uâ‚– - Î£â±¼â‚Œâ‚áµâ»Â¹ proj_vâ±¼(uâ‚–)`
- [ ] **Application**: Orthonormal basis construction

### Spectral Theorem
- [ ] **Equation**: `A = QÎ›Qáµ€` (for symmetric A)
- [ ] **Application**: Eigendecomposition

### Positive Definite Matrix
- [ ] **Equation**: `xáµ€Ax > 0` for all `xâ‰ 0`
- [ ] **Application**: Valid distance metrics

---

## ğŸ“¡ Information Theory

### Entropy
- [ ] **Equation**: `H(X) = -Î£P(x)log P(x)`
- [ ] **Application**: Uncertainty measure

### Cross-Entropy
- [ ] **Equation**: `H(p,q) = -Î£P(x)log q(x)`
- [ ] **Application**: Classification loss function

### KL Divergence
- [ ] **Equation**: `D_KL(p||q) = Î£P(x)log[P(x)/q(x)]`
- [ ] **Application**: Policy constraints (PPO, TRPO)

### Mutual Information
- [ ] **Equation**: `I(X;Y) = H(X) - H(X|Y)`
- [ ] **Application**: Representation learning

### Fisher Information
- [ ] **Equation**: `I(Î¸) = E[(âˆ‚log p(x|Î¸)/âˆ‚Î¸)Â²]`
- [ ] **Application**: Natural gradient methods

---

## ğŸ”„ Dynamical Systems

### State-Space Representation
- [ ] **Equation**: `áº‹ = f(x,u)`, `y = h(x)`
- [ ] **Application**: Robot system modeling

### Linearization
- [ ] **Equation**: `áº‹ â‰ˆ Ax + Bu` where `A=âˆ‚f/âˆ‚x`, `B=âˆ‚f/âˆ‚u`
- [ ] **Application**: Local linear approximation

### Lyapunov Stability
- [ ] **Equation**: `V(x) > 0`, `VÌ‡(x) < 0`
- [ ] **Application**: Stability proof

### Transfer Function
- [ ] **Equation**: `G(s) = Y(s)/U(s)`
- [ ] **Application**: Frequency domain analysis

### Controllability Matrix
- [ ] **Equation**: `C = [B AB AÂ²B ... Aâ¿â»Â¹B]`
- [ ] **Application**: Full controllability check

### Observability Matrix
- [ ] **Equation**: `O = [C; CA; CAÂ²; ...; CAâ¿â»Â¹]`
- [ ] **Application**: State estimation feasibility

---

## ğŸ² Markov Processes

### Markov Property
- [ ] **Equation**: `P(sâ‚œâ‚Šâ‚|sâ‚œ,sâ‚œâ‚‹â‚,...) = P(sâ‚œâ‚Šâ‚|sâ‚œ)`
- [ ] **Application**: Memoryless state transitions

### Transition Probability Matrix
- [ ] **Equation**: `P(s'|s,a)`
- [ ] **Application**: State dynamics modeling

### Stationary Distribution
- [ ] **Equation**: `Ï€P = Ï€`
- [ ] **Application**: Long-term behavior

### Bellman Equation
- [ ] **Equation**: `V(s) = maxâ‚[R(s,a) + Î³Î£â‚›'P(s'|s,a)V(s')]`
- [ ] **Application**: Optimal value function

### Q-Function
- [ ] **Equation**: `Q(s,a) = R(s,a) + Î³Î£â‚›'P(s'|s,a)V(s')`
- [ ] **Application**: Action-value estimation

### Policy Iteration
- [ ] **Equation**: `Ï€â‚–â‚Šâ‚(s) = argmaxâ‚ Q^Ï€â‚–(s,a)`
- [ ] **Application**: Policy improvement

---

## ğŸ”¢ Numerical Methods

### Newton-Raphson Method
- [ ] **Equation**: `xâ‚™â‚Šâ‚ = xâ‚™ - f(xâ‚™)/f'(xâ‚™)`
- [ ] **Application**: Root finding, inverse kinematics

### Bisection Method
- [ ] **Equation**: If `f(a)f(b)<0`, root exists in `[a,b]`
- [ ] **Application**: Robust root finding

### Linear Interpolation
- [ ] **Equation**: `y = yâ‚€ + (yâ‚-yâ‚€)(x-xâ‚€)/(xâ‚-xâ‚€)`
- [ ] **Application**: Trajectory smoothing

### Spline Interpolation
- [ ] **Equation**: Piecewise polynomials with continuity
- [ ] **Application**: Smooth path generation

### Forward Difference
- [ ] **Equation**: `f'(x) â‰ˆ [f(x+h)-f(x)]/h`
- [ ] **Application**: Numerical derivatives

### Trapezoidal Rule
- [ ] **Equation**: `âˆ«â‚áµ‡f(x)dx â‰ˆ (h/2)[f(xâ‚€)+2f(xâ‚)+...+f(xâ‚™)]`
- [ ] **Application**: Numerical integration

---

## ğŸ“ Advanced Calculus

### Functional
- [ ] **Equation**: `J[y] = âˆ«L(x,y,y')dx`
- [ ] **Application**: Path cost optimization

### Euler-Lagrange Equation
- [ ] **Equation**: `d/dt(âˆ‚L/âˆ‚qÌ‡) - âˆ‚L/âˆ‚q = 0`
- [ ] **Application**: Equations of motion

### Lagrangian Mechanics
- [ ] **Equation**: `L = T - V` (kinetic - potential energy)
- [ ] **Application**: Energy-based dynamics formulation

### Hamiltonian
- [ ] **Equation**: `H = Î£páµ¢qÌ‡áµ¢ - L`
- [ ] **Application**: Energy-based optimal control

### Principle of Least Action
- [ ] **Equation**: `Î´S = Î´âˆ«L dt = 0`
- [ ] **Application**: Optimal trajectory derivation

---

## ğŸ² Stochastic Processes

### Wiener Process (Brownian Motion)
- [ ] **Equation**: `dW ~ N(0,dt)`
- [ ] **Application**: Continuous random walk modeling

### Stochastic Differential Equation
- [ ] **Equation**: `dx = f(x,t)dt + g(x,t)dW`
- [ ] **Application**: Stochastic system dynamics

### ItÃ´'s Lemma
- [ ] **Equation**: `df = (âˆ‚f/âˆ‚t + Î¼âˆ‚f/âˆ‚x + Â½ÏƒÂ²âˆ‚Â²f/âˆ‚xÂ²)dt + Ïƒâˆ‚f/âˆ‚x dW`
- [ ] **Application**: Stochastic chain rule

### Ornstein-Uhlenbeck Process
- [ ] **Equation**: `dx = Î¸(Î¼-x)dt + ÏƒdW`
- [ ] **Application**: Mean-reverting noise model

### Martingale Property
- [ ] **Equation**: `E[Xâ‚œâ‚Šâ‚|Xâ‚,...,Xâ‚œ] = Xâ‚œ`
- [ ] **Application**: Unbiased value estimation

---

## ğŸŒ Differential Geometry (Advanced Robotics)

### Manifold Charts
- [ ] **Equation**: `Ï†: M â†’ â„â¿`
- [ ] **Application**: Local coordinate systems

### Tangent Space
- [ ] **Equation**: `Tâ‚šM`
- [ ] **Application**: Velocity representations

### Lie Groups
- [ ] **Equation**: `SO(3)` rotation matrices, `SE(3)` rigid transforms
- [ ] **Application**: 3D rotations and poses

### Exponential Map
- [ ] **Equation**: `exp: ğ”°ğ”¬(3) â†’ SO(3)`
- [ ] **Application**: Skew-symmetric to rotation matrix

### Geodesic
- [ ] **Equation**: `âˆ‡_Î³Ì‡Î³Ì‡ = 0`
- [ ] **Application**: Shortest path on manifold

### Riemannian Metric
- [ ] **Equation**: `dsÂ² = gáµ¢â±¼dxâ±dxÊ²`
- [ ] **Application**: Distance on curved spaces

---

## ğŸ“š Additional Resources

### Recommended Textbooks
- Linear Algebra: "Introduction to Linear Algebra" by Gilbert Strang
- Calculus: "Calculus" by James Stewart
- Probability: "Introduction to Probability" by Blitzstein & Hwang
- Optimization: "Convex Optimization" by Boyd & Vandenberghe
- Robotics: "Modern Robotics" by Lynch & Park
- RL: "Reinforcement Learning: An Introduction" by Sutton & Barto

### Online Resources
- Khan Academy (Foundations)
- 3Blue1Brown (Visual Intuition)
- MIT OpenCourseWare
- Stanford CS229, CS231n, CS234

--
